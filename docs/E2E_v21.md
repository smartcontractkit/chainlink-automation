# E2E offchain protocol overview v2.1

This doc aims to give a high level overview of a full e2e protocol for automation `v2.1` (conditional + log trigger upkeeps). The protocol aims to give the following guarantees

- **Reliability guarantees**:
    - Log triggers: Out of n=10 nodes every node listens to configured user log, as soon f+1=3 nodes see the log and agree on checkPipeline, it will be performed on chain. We can handle up to 7 nodes missing a log and handle X (~10) log trigger upkeeps with up to 5 logs per second
    - Conditionals: Every upkeepâ€™s condition will be checked at least once by the network every Y (~3) seconds, with up to f+1=3 nodes being down. As soon as f+1=3 nodes agree on the checkPipeline, it will be performed on chain. We can handle up to Z (~500) conditional upkeeps
    - The protocol will be functional as long as > (n+f)/2 = 6 nodes are alive within the p2p network (required for ocr3 consensus)
- **Security guarantees**: At least f+1=3 independent nodes need to achieve agreement on an upkeep and its data before it is performed

## High level Overview of Protocol

### Definitions

- `upkeepID`: Unique 256 bit identifier for an upkeep. Each upkeep has a unique trigger type (conditional or log) which is encoded within the ID
- `trigger`: Used to represent the trigger for a particular upkeep performance
    - For conditionals: (checkBlockNum, checkBlockHash)
    - For log triggers: (logTxHash, logIndex, blockNum, blockHash)
- `upkeepTriggerID`: Uniquely identifies an attempt to do a unit of work and is represented as:
`keccak256(upkeepID, trigger)`
- `logIdentifier`: unique identifier for a log â†’ (logTxHash, logIndex)
- `upkeepPayload`: Input information to process an upkeep â†’ (upkeepID, trigger, checkData)
    - For conditionals checkData is empty (derived onchain in checkUpkeep)
    - For log: checkData is log information
- `upkeepResult`: Output information to perform an upkeep. Same across both types: (fastGasWei, linkNative, upkeepID, trigger, gasLimit, performData)

## [WIP] Visuals

Source of truth here: https://miro.com/app/board/uXjVPntyh4E=/

Conditional triggers flow:

![Screenshot 2023-07-11 at 21.21.12.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/03c59822-6b46-4bea-a03b-f9d8b60527df/Screenshot_2023-07-11_at_21.21.12.png)

Log triggers flow:

![Screenshot 2023-07-11 at 21.24.59.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1e2870dc-71cf-4d4b-838d-8495352c2336/Screenshot_2023-07-11_at_21.24.59.png)

A generalized diagram for both flows, with abstract components:

![Screenshot 2023-07-11 at 21.34.44.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b2c6c166-7ee8-40c7-9b23-eb6219a36889/Screenshot_2023-07-11_at_21.34.44.png)

### Components

**Registry**

This is the main component that connects the offchain node with the registry onchain. The main functionality it offers

- Sync the upkeep config onchain with nodeâ€™s local state in the upkeep index
    - Listens to upkeep events on chain for log trigger upkeep events
    - Periodically (~10min) does a fullSync of upkeep onchain state so that any potential missed / reorged logs are accounted for
- Expose a checkPipeline to call checkUpkeep
    - Takes in a list of upkeepPayloads to process
    - For log triggers verifies log is still part of chain at checkBlockNum
    - Does a batch RPC call to checkUpkeep (for conditionals calls `checkUpkeep()` and for log calls `checkUpkeep(logData)`)
    - Does batch mercury fetch and callback for upkeeps that need mercury data
    - Does batch RPC call to simulatePerformUpkeep for upkeeps that are eligible
    - Returns list of results for each payload. Result can be **eligible** with upkeepResult / **non-eligible** with failure reason / **error.** Error can be retryable if itâ€™s transient or non retryable in case the checkBlockNum is too old

**Upkeep Index (Life Cycle)**

This component maintains the config for an upkeep in memory so that it can be read by different components.

- Called by registry upon any changes (or potentially without any change during a fullSync)
- Stores a list of active upkeepIDs and their associated offchain config
- For log upkeeps, stores the log filter config and the block number from when it is active. Upon any changes to the log filter config does a call to log event provider

**Log Provider**

This componentâ€™s purpose is to surface latest logs for registered upkeeps. It does not maintain any state across restarts (no DB). The main functionality it exposes

- Listening for log filter config changes from Upkeep Index and syncing log poller with the filters
- Provides a simple interface `getLatestLogs` to provide new **unseen** logs across all upkeeps within a limit
    - Repeatedly queries latest logs from the chain (via log poller DB) for the last `lookbackBlocks` (~200) blocks. Stores them in the log buffer (see below)
    - Handles load balancing and rate limiting across upkeeps
    - `getLatestLogs` limits the number of logs returned per upkeep in a single call. If there are more logs present, then the provider gives the logs starting from an offset (`latestBlock-latestBlock%lookbackBlocks`). Offset is calculated such that the nodes try to choose the same logs from big pool of logs so they can get consensus

<aside>
ðŸ’¡ Note: `getLatestLogs` might miss logs when there is a surge of logs which lasts longer than `lookbackBlocks`. Upon node restarts it can **miss logs** when it restarts after a gap, or **provide same logs again** when it quickly restarts
</aside>

- Provides an interface `getPayloadLog` to build an upkeep payload for a particular log on demand by giving a trigger as in input.
It gets only the required log from the log buffer or reads it from log poller if not found in buffer. This is used by the recovery flow

**Log Buffer**

A circular/ring buffer of blocks and their corresponding logs, that act as a cache for logs. 
It is used to store the last `lookbackBlocks*3` blocks, where each block holds a list of that block's logs. 
Logs are marked as seen when they are returned by the buffer, to avoid working with logs that have already been seen.

It is used by the log provider to provide unknown logs to the node, and by by the log recoverer to identify known logs during recovery flow.

**Runner**

This component is responsible for parallelizing upkeep executions and providing a single interface to different components maintaining a shared cache

- Takes a list of upkeepPayloads, calls CheckPipeline asynchronously with upkeeps being batched in a single pipeline execution
- Maintains in-memory cache of non-errored pipeline executions indexed on (trigger, upkeepID). Directly uses that result instead of a new execution if available
- Allows for repeated calls for the same (trigger, upkeepID). If an execution is in progress then it doesnâ€™t start a new execution
- Execution automatically fails after configured timeout. (~10s)
- Does a callback when results are available with the result

<aside>
ðŸ’¡ Note: This component can be made synchronous instead of giving a callback while callers can call it asynchronously

</aside>

**Coordinator**

This component stores in-flight & confirmed reports in memory. It does not maintain any state in DB for itself.

- Stores the trigger, upkeepID and triggerID for a report which is inflight (called via plugin shouldAccept)
    - For conditionals only one inflight report should be present per upkeepID
    - For log upkeeps only one inflight report per (upkeepID, logIdentifier)
    - If a new report is seen for an upkeepID / (upkeepID, logIdentifier) then it waits on the higher checkBlockNumber report
- Listens to upkeepPerformed / StaleUpkeep (stale / reorged / cancelled / insufficient funds) logs for stored triggerIDs in memory
    - In case of upkeepPerformed log: (trigger, upkeep) is marked as performed at the log block number for conditionals
    - Write to DB (trigger, upkeepID) as being performed (used within recovery flow)
    - In case of staleUpkeep log
        - For conditional: Marked as stale at the log block number. It will be sampled and checked again automatically
        - For log (only for reorged, insufficient funds reason): Put (upkeepID, logTxHash, logIndex) into recovery flow
- All keys stored expire after TTL. In case it was not marked as performed, it is assumed tx got lost
    - For conditional: Just remove from memory. It will be sampled and checked again automatically
    - For log: Put (upkeepID, logTxHash, logIndex) into recovery flow
- Provides readAPI to check whether (trigger, upkeepID) is in flight. (Used within shouldTransmit)
- Provides additional read API to check whether an upkeep should be processed which is used for pre-processing / filtering to prevent duplicate reports. **Note that the input is different from trigger and doesnâ€™t include checkBlockNumber, checkBlockHash**. i.e.
    - For conditional: shouldProcess(upkeepID, blockNum)
        - False if report is inflight for upkeepID
        - False if report has been confirmed after blockNum (This can happen when network latest block is lagging this nodeâ€™s logs)
        - True otherwise
    - For log: shouldProcess(upkeepID, logIdentifier)
        - False if report is inflight or has been confirmed
        - True otherwise

**Result Store**

This component is responsible for storing upkeepResults that a node thinks should be performed. It hopes to get consensus from the network on these results to push into reports. Best effort is made to ensure the same logs enter different nodeâ€™s result stores independently as **it is assumed that blockchain nodes will get in sync within TTL**, but it is not guaranteed as nodeâ€™s local blockchain can see different reorgs or select different logs during surges. For results that do not achieve consensus within TTL go into recovery flow.

- Maintains an in-memory collection of upkeepResults. It should maintain a single result per `upkeepTriggerID``. If it gets a same result for the identifier it overwrites it.
- Each result has a TTL. When the TTL expires
    - For conditional: just remove from memory. It will be sampled and checked again
    - For log: Put (upkeepID, logTxHash, logIndex) into recovery flow
- Provides a read API which takes as input (`pseudoRandomSeed`, `limit`). Sorts all keys (trigger, upkeepID) with the `seed` and provides first `limit` results. We do not do FIFO here as potentially out of sync old results will block new results sent by the node for consensus.
- Provides an API to remove (trigger, upkeepID) from the store

**Metadata Store**

This component stores the metadata to be sent within observations to the network. There are three categories of metadata

- Conditional Upkeep Instructions
- Log recovery Instructions
- Latest block history
- Provides an add / view / remove API for other components
- Every item has a TTL. Upon expiry items are just removed without any action

**Samples Observer**

- On a constant time interval (via Sampler ticker) fetch a sample of current conditional upkeep IDs from registry. Use local latest block number as trigger.
- Filter the sampled IDs from coordinator shouldProcess(upkeepID, latestBlock)
- Calls runner on the list of upkeepIDs and latestBlock
- In callback of runner if upkeep is eligible then put upkeepID into metadata store (Conditional Upkeep Instructions)

**Conditional Observer**

- Gets coordinated block + upkeep payload from plugin
- Pre-processes to filter upkeep present in coordinator
- Calls runner with upkeep payload
- If upkeep is eligible enqueue into result store
- Errors and ineligible results are ignored

**Log Observer**

- Calls `getLatestLogs` every second (via Log Trigger Ticker). Gets the full upkeep payload as input
- If log block number is older than threshold then put log into recovery flow, else continue
- Pre-processes to filter the logs already present in coordinator
- Calls runner with upkeep payload
- If upkeep is eligible enqueue into result store
- If runner gave an error, put back into log ticker after a fixed timeout (can extend to exponential backoff)

<aside>
ðŸ’¡ Note: This starts with a simple retry mechanism. Retries will automatically end once log goes into recovery flow
</aside>

<aside>
ðŸ’¡ Note: Duplicate logs can enter this flow, this component should be able to handle that. This component can maintain in-memory state for processed logs and can also use that state for exponential backoff
</aside>

**Recovery Observer**

This flow gets as input an upkeepID and a logIdentifier. It is a backup flow which verifies that a log was fully processed. Logs can be put here from the node itself (through Log Recoverer) or from other nodes (through Plugin)

- Input is an upkeepID, logTxHash, logIndex (optional checkBlockNum, checkBlockHash when passed from plugin)
- Checks whether log should be processed
    - Whether its part of the filters for that upkeep and it is still present on chain (DB call to log poller)
    - Whether it is older from the latest block within a threshold (To prevent malicious node requesting recovery on a new log)
- Checks whether the log has been already processed
    - If node locally checked (logTxHash, logIndex) and got a non eligible result then skip (DB call)
    - If chain has a perform for the log then skip (DB call which looks at upkeepPerformed logs)
- If checkBlockNum not present: puts into an **in-memory recovery queue** which is then read in the plugin and put back in the flow with a coordinated checkBlock
- If checkBlockNum present
    - get payload from log provider for (upkeepID, logTxHash, logIndex)
    - call runner with payload
    - upon result, if eligible add into result store
    - non-eligible are written to DB
    - If runner gave an error then it is ignored (it might get picked up again by log recoverer, but we do not keep retrying here)

<aside>
ðŸ’¡ Note: Duplicate logs can enter this flow, this component should be able to handle that

</aside>

**Log Recoverer**

The log recoverer is responsible to ensure that no logs are missed.
It does that by running a background process for scanning logs and putting the ones we missed into the recovery flow (without checkBlockNum/Hash).

While the provider is scanning latest logs, the recoverer is scanning older logs in ascending order, up to `latestBlock - lookbackBlocks`, newer blocks will be under the provider's lookback window.

The recoverer maintains a `lastRePollBlock` for each upkeep, .i.e. the last block it scanned for that upkeep. It will start scanning from that block on each iteration, and update the block number when it finishes scanning.
Logs that are older than 24hr are ignored, therefore `lastRePollBlock` starts at `latestBlock - (24hr block)` in case not populated.

The recoverer will scan logs every second, for a subset of `n` upkeeps, where `n/2` upkeeps are randomly chosen and `n/2` upkeeps are chosen by the oldest `lastRePollBlock`.

**Upkeep States**

The upkeeps states are used to track the status of log upkeeps (eligible, performed) across the system,
to avoid redundant work by the recoverer.

The states will be persisted to so the latest state to be restored when the node starts up.

**Plugin**

- Observation
    - From previous outcome
        - Remove agreed upon finalized results from result store
        - For `acceptedSamples` (bound to a trigger)
            - Remove from metadata store
            - Enqueue (trigger, upkeepID) into conditional observer
        - For `acceptedRecoveryLogs` (bound to a trigger)
            - Remove from metadata store
            - Enqueue (trigger, upkeepID) into recovery observer
    - Query results from result store giving seqNr as pseudoRandom seed and predefined limit. Filter results using coordinator and add them to observation
    - Query from metadata store for sample and recovery instructions using seqNr as pseudoRandom seed within limits, filter using coordinator and add them to observation
    - Query latest block number and hashes from local chain, add them to observation
- Outcome
    - Derive latest blockNumber and blockHash
    - Any result which has f+1 agreement is added to finalized result
    - All samples are collected from observations within limits, deduped and filtered from existing `acceptedSamples`. These are then added to `acceptedSamples` in the outcome bound to the current latestBlockNumber and hash.
        - `acceptedSamples` is a ring buffer where samples are held for â€˜xâ€™ (~30) rounds so that they do not get bound to a new blockNumber for some time
    - Similar behaviour as samples is done for recovery logs to maintain `acceptedRecoveryLogs`
- Reports
    - From finalised results from outcome, package them into reports with potential batching of upkeeps
        - Batching is subject to upkeep gas limit, and preconfigured reportGasLimit and gasOverheadPerUpkeep
        - Additionally same upkeep ID is not batched within the same report
        - For a list of upkeepResults, we only need to send one fastGasWei, linkNative to chain in the report. This is taken from the result which has the highest checkBlockNum
- ShouldAccept
    - Extract [(trigger, upkeepID)] from report and add to coordinator
    - Always return true

<aside>
ðŸ’¡ Note: We cannot guarantee that the same (upkeepID) / (logIdentifier, upkeepID) will not be already existing in coordinator. (e.g. nodeâ€™s local chain is lagging the network). We need to have an override behaviour where we wait on the higher checkBlockNum report.

</aside>

- ShouldTransmit
    - Extract [(trigger, upkeepID)] from report. If any (trigger, upkeepID) is not yet confirmed then return true